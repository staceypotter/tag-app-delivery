<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://cncf.github.io/tag-app-delivery/feed.xml" rel="self" type="application/atom+xml" /><link href="https://cncf.github.io/tag-app-delivery/" rel="alternate" type="text/html" /><updated>2022-09-12T15:52:19+00:00</updated><id>https://cncf.github.io/tag-app-delivery/feed.xml</id><title type="html">CNCF TAG App Delivery</title><entry><title type="html">Kubecon NA TAG App Delivery Project Meeting and Booth</title><link href="https://cncf.github.io/tag-app-delivery/general/2022/09/12/kubeconna-project-meeting.html" rel="alternate" type="text/html" title="Kubecon NA TAG App Delivery Project Meeting and Booth" /><published>2022-09-12T11:04:00+00:00</published><updated>2022-09-12T11:04:00+00:00</updated><id>https://cncf.github.io/tag-app-delivery/general/2022/09/12/kubeconna-project-meeting</id><content type="html" xml:base="https://cncf.github.io/tag-app-delivery/general/2022/09/12/kubeconna-project-meeting.html"><![CDATA[<h1 id="kubecon-na-tag-app-delivery-schedule">Kubecon NA TAG App Delivery Schedule</h1>

<h2 id="project-meeting">Project Meeting</h2>

<p>Tuesday, October 25th: 1:00PM - 5:00PM ET</p>

<h2 id="tag-app-delivery-booth-times">TAG App Delivery Booth times</h2>

<p>Wednesday, October 26: 10:30AM - 3:30PM ET
Thursday, October 27: 10:30AM - 2:30PM ET
Friday, October 27: 10:30AM - 1:00PM ET</p>

<h2 id="call-for-proposals-for-project-meeting">Call for Proposals for Project Meeting</h2>

<p>Presentations will be 15 minutes and should cover <em>CNCF projects or end-user use cases</em>. The submission of talks for <em>sandbox projects is highly encouraged</em>.</p>

<p>Please submit your proposal here https://forms.gle/wigUmKG7paMPfHPM8</p>

<h3 id="tag-app-delivery-booth">TAG App Delivery Booth</h3>

<p>Come to our booth for Q&amp;As about our working groups and to the TAG Tech Leads and chairs if you’d like to know more about the TAG or would like feedback on your project’s sandbox submission, or just to say hi, of course :).</p>]]></content><author><name>Jennifer Strejevitch</name></author><category term="general" /><summary type="html"><![CDATA[Kubecon NA TAG App Delivery Schedule]]></summary></entry><entry><title type="html">Clusters for all cloud tenants</title><link href="https://cncf.github.io/tag-app-delivery/multi-tenancy/2022/06/02/clusters-for-all-cloud-tenants.html" rel="alternate" type="text/html" title="Clusters for all cloud tenants" /><published>2022-06-02T11:04:00+00:00</published><updated>2022-06-02T11:04:00+00:00</updated><id>https://cncf.github.io/tag-app-delivery/multi-tenancy/2022/06/02/clusters-for-all-cloud-tenants</id><content type="html" xml:base="https://cncf.github.io/tag-app-delivery/multi-tenancy/2022/06/02/clusters-for-all-cloud-tenants.html"><![CDATA[<h1 id="clusters-for-all-cloud-tenants">Clusters for all cloud tenants</h1>

<p>A decision which faces many large organizations as they adopt cloud architecture is how to provide isolated spaces within the same environments and clusters for various teams and purposes. For example, marketing and sales applications may need to be isolated from an organization’s customer-facing applications; and development teams building any app usually require extra spaces for tests and verification.</p>

<h2 id="namespace-as-unit-of-tenancy">Namespace as unit of tenancy</h2>

<p>To address this need, many organizations have started to use namespaces as units of isolation and tenancy, a pattern previously described by <a href="https://cloud.google.com/kubernetes-engine/docs/concepts/multitenancy-overview">Google</a> and <a href="https://kubernetes.io/blog/2021/04/15/three-tenancy-models-for-kubernetes/">Kubernetes contributors</a>. But namespace-scoped isolation is often insufficient because some concerns are managed at cluster scope. In particular, installing new resource types (CRDs) is a cluster-scoped activity; and today independent teams often want to install custom resource types and operators. Also, more developers are themselves writing software operators and custom resource types and find themselves requiring cluster-scoped access for research and tests.</p>

<h2 id="cluster-as-unit-of-tenancy">Cluster as unit of tenancy</h2>

<p>For these reasons and others, tenants often require their own isolated clusters with unconstrained access rights. In an isolated cluster, a tenant gets its own Kubernetes API server and persistence store and fully manages all namespaces and custom resource types in its cluster.</p>

<p>But deploying physical or even virtual machines for many clusters is inefficient and difficult to manage, so organizations have struggled to provide clusters to tenant teams. Happily :smile:, to meet these organizations’ and users’ needs, leading Kubernetes vendors have been researching and developing lighter weight mechanisms to provide isolated clusters for an organization’s tenants. In this post we’ll compare and contrast several of these emergent efforts.</p>

<p>Do you have other projects and ideas to enhance multitenancy for cloud architecture? Then please join CNCF’s App Delivery advisory group in discussing these <a href="https://github.com/cncf/tag-app-delivery/issues/193">here</a>; thank you!</p>

<h3 id="vcluster">vcluster</h3>

<p><a href="https://www.vcluster.com/">vcluster</a> is <a href="https://www.google.com/search?q=vcluster&amp;tbm=nws">a prominent project</a> and CLI tool maintained by <a href="https://loft.sh/">loft.sh</a> that provisions a virtual cluster as a StatefulSet within a tenant namespace. Access rights from the hosting namespace are propogated to the hosted virtual cluster such that the namespace tenant becomes the cluster’s only tenant. As cluster admins, tenant members can create cluster-scoped resources like CRDs and ClusterRoles.</p>

<p>The virtual cluster runs its own Kubernetes API service and persistence store independent of those of the hosting cluster. It can be published by the hosting cluster as a LoadBalancer-type service and accessed directly with kubectl and other Kubernetes API-compliant tools. This enables users of the tenant cluster to work with it directly with little or no knowledge of its host.</p>

<p>In vcluster and the following solutions, the virtual cluster is a “metadata-only” cluster, in that resources in it are persisted to a backing store like etcd, but no schedulers act to reify the persisted resources - ultimately as pods. Instead, a “syncer” synchronization service copies and transforms reifiable resources - podspecs - from the virtual cluster to the hosting namespace of the hosting cluster. Schedulers in the hosting cluster then detect and reify these resources in the same underlying tenant namespace where the virtual cluster’s control plane runs.</p>

<p>An advantage of vcluster’s approach of scheduling pods in the hosting namespace is that the hosting cluster ultimately handles all workloads and applies namespace quotas - all work happens within the namespace allocated to the tenant by the hosting cluster administrator. A disadvantage is that schedulers cannot be configured in the virtual cluster since pods aren’t actually run there.</p>

<ul>
  <li><a href="https://github.com/loft-sh/vcluster">vcluster on GitHub</a></li>
</ul>

<h3 id="cluster-api-provider-nested-capn">Cluster API Provider Nested (CAPN)</h3>

<p>In vcluster, bespoke support for control plane implementations is required; as of this writing, vcluster supports k3s, k0s and vanilla k8s distributions.</p>

<p>To support <em>any</em> control plane implementation, the <a href="https://github.com/kubernetes-sigs/cluster-api-provider-nested">Cluster API Provider Nested</a> project implements an architecture similar to that of vcluster, including a metadata-only cluster and a syncer, but provisions the control plane using a Cluster API provider rather than a bespoke distribution.</p>

<p>CAPN promises to enable control planes implementable via Cluster API to serve virtual clusters.</p>

<h3 id="hypershift">HyperShift</h3>

<p>Similar to the previous two, <a href="https://www.redhat.com/">Red Hat</a>’s <a href="https://github.com/openshift/hypershift">HyperShift</a> project provisions an OpenShift (Red Hat’s Kubernetes distro) control plane as a collection of pods in a host namespace. But rather than running workloads within the hosting cluster and namespace like vcluster, HyperShift control planes are connected to a pool of dedicated worker nodes where pods are synced and scheduled.</p>

<p>HyperShift’s model may be most appropriate for a hosting provider like Red Hat which desires to abstract control plane management from their customers and allow them to just manage worker nodes.</p>

<h3 id="kcp">kcp</h3>

<p>Finally, <a href="https://github.com/kcp-dev/kcp">kcp</a> is another proposal and project from <a href="https://www.redhat.com/">Red Hat</a> inspired by and reimagined from all of the previous ideas. Whereas the above virtual clusters run <em>within</em> a host cluster and turn to the host cluster to run pods, manage networks and provision volumes, kcp reverses this paradigm and makes the <em>hosting</em> cluster a metadata-only cluster. <em>Child</em> clusters - <em>workspaces</em> in the kcp project - are registered with the hub metadata-only cluster and work is delegated to these children based on labels on resources in the hub.</p>

<p>As opposed to hosted virtual clusters, child clusters in kcp <em>could</em> manage their own schedulers. Another advantage of kcp’s paradigm inversion is centralized awareness and management of child clusters. In particular, this enables simpler centralized policies and standards for custom resource types to be propogated to all children.</p>

<h2 id="conclusion">Conclusion</h2>

<p>vcluster, CAPN, HyperShift, and kcp are emerging projects and ideas to meet cloud users’ needs for multitenancy with <em>clusters</em> as the unit of tenancy. Early adopters are already providing feedback on good and better parts of these approaches and new ideas emerge daily.</p>

<p>Want to help drive new ideas for cloud multitenancy? Want to help cloud users understand and give feedback on emerging paradigms in this domain? Then join <a href="https://github.com/cncf/tag-app-delivery/issues/193">the discussion</a> in CNCF’s TAG App Delivery. Thank you!</p>]]></content><author><name>Josh Gavant</name></author><category term="multi-tenancy" /><summary type="html"><![CDATA[Clusters for all cloud tenants]]></summary></entry></feed>